import os
from typing import List, Dict, Optional
from dataclasses import dataclass
from pathlib import Path

import numpy as np
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI  # Обновленные импорты
from langchain_community.vectorstores import Chroma
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_community.document_loaders import (
    PyPDFLoader,
    UnstructuredWordDocumentLoader,
    TextLoader
)

@dataclass
class Document:
    content: str
    metadata: Dict

class DocumentProcessor:
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
    
    def load_document(self, file_path: str) -> List[Document]:
        file_extension = Path(file_path).suffix.lower()
        
        if file_extension == '.pdf':
            loader = PyPDFLoader(file_path)
        elif file_extension in ['.docx', '.doc']:
            loader = UnstructuredWordDocumentLoader(file_path)
        elif file_extension == '.txt':
            loader = TextLoader(file_path)
        else:
            raise ValueError(f"Unsupported file type: {file_extension}")
        
        documents = loader.load()
        return documents
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        return self.text_splitter.split_documents(documents)

class VectorStore:
    def __init__(self, embedding_model: OpenAIEmbeddings):
        self.embedding_model = embedding_model
        self.vector_store = None
        
    def add_documents(self, documents: List[Document], collection_name: str):
        if self.vector_store is None:
            self.vector_store = Chroma.from_documents(
                documents=documents,
                embedding=self.embedding_model,
                collection_name=collection_name
            )
        else:
            self.vector_store.add_documents(documents)
    
    def similarity_search(self, query: str, k: int = 4) -> List[Document]:
        return self.vector_store.similarity_search(query, k=k)

class ChatBot:
    def __init__(
        self,
        vector_store: VectorStore,
        model_name: str = "gpt-3.5-turbo",
        temperature: float = 0.7
    ):
        self.vector_store = vector_store
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )
        
        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=ChatOpenAI(model_name=model_name, temperature=temperature),
            retriever=self.vector_store.vector_store.as_retriever(),
            memory=self.memory,
            return_source_documents=True
        )
    
    def get_answer(self, question: str) -> Dict:
        result = self.qa_chain({"question": question})
        
        return {
            "answer": result["answer"],
            "source_documents": result["source_documents"],
            "chat_history": self.memory.chat_memory.messages
        }

class RAGSystem:
    def __init__(
        self,
        openai_api_key: str,
        chunk_size: int = 1000,
        chunk_overlap: int = 200
    ):
        os.environ["OPENAI_API_KEY"] = openai_api_key
        
        self.document_processor = DocumentProcessor(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        
        self.vector_store = VectorStore(
            embedding_model=OpenAIEmbeddings()
        )
        
        self.chatbot = ChatBot(
            vector_store=self.vector_store
        )
    
    def process_document(self, file_path: str, collection_name: str):
        # Load document
        documents = self.document_processor.load_document(file_path)
        
        # Split into chunks
        chunks = self.document_processor.split_documents(documents)
        
        # Add to vector store
        self.vector_store.add_documents(chunks, collection_name)
        
        return len(chunks)
    
    def ask_question(self, question: str) -> Dict:
        return self.chatbot.get_answer(question)

# Example usage
if __name__ == "__main__":
    # Initialize the RAG system
    rag_system = RAGSystem(
        openai_api_key="your-openai-api-key-here"
    )
    
    # Process a document
    num_chunks = rag_system.process_document(
        file_path="example.pdf",
        collection_name="my_documents"
    )
    print(f"Processed document into {num_chunks} chunks")
    
    # Ask questions
    question = "What is the main topic of the document?"
    result = rag_system.ask_question(question)
    
    print(f"Question: {question}")
    print(f"Answer: {result['answer']}")
    print("Sources:", [doc.metadata for doc in result['source_documents']])